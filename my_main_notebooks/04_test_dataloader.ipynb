{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"my_data\",  # where to download data to?\n",
    "    train=True,  # get training data\n",
    "    download=True,  # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(),  # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None,  # you can transform labels as well\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.mnist.FashionMNIST,\n",
       " torch.utils.data.dataloader.DataLoader)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data), type(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i[0].shape)\n",
    "    break\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 1, 28, 28]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(batch, X.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "train_dir = image_path / 'train'\n",
    "# Write transform for image\n",
    "data_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    # p = probability of flip, 0.5 = 50% chance\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # Turn the image into a torch.Tensor\n",
    "    # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir,  # target folder of images\n",
    "                                  # transforms to perform on data (images)\n",
    "                                  transform=data_transform,\n",
    "                                  target_transform=None)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=1,  # how many samples per batch?\n",
    "                              # how many subprocesses to use for data loading? (higher = more)\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)  # shuffle the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 225\n",
       "    Root location: data\\pizza_steak_sushi\\train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4235, 0.4157, 0.4039,  ..., 0.0392, 0.0471, 0.0392],\n",
      "          [0.6431, 0.6392, 0.6431,  ..., 0.0392, 0.0431, 0.0392],\n",
      "          [0.7843, 0.7843, 0.7922,  ..., 0.0510, 0.0471, 0.0392],\n",
      "          ...,\n",
      "          [0.7647, 0.7765, 0.7961,  ..., 0.2706, 0.2471, 0.3490],\n",
      "          [0.7569, 0.7686, 0.7922,  ..., 0.2667, 0.2353, 0.2549],\n",
      "          [0.7137, 0.7451, 0.7725,  ..., 0.3020, 0.2824, 0.2392]],\n",
      "\n",
      "         [[0.5137, 0.5137, 0.5255,  ..., 0.0392, 0.0510, 0.0510],\n",
      "          [0.7137, 0.7176, 0.7137,  ..., 0.0510, 0.0588, 0.0510],\n",
      "          [0.8196, 0.8196, 0.8196,  ..., 0.0627, 0.0627, 0.0549],\n",
      "          ...,\n",
      "          [0.8000, 0.8235, 0.8392,  ..., 0.0471, 0.0392, 0.0471],\n",
      "          [0.7725, 0.8078, 0.8275,  ..., 0.0431, 0.0392, 0.0353],\n",
      "          [0.7255, 0.7647, 0.8039,  ..., 0.0627, 0.0549, 0.0353]],\n",
      "\n",
      "         [[0.5529, 0.5922, 0.5843,  ..., 0.0353, 0.0431, 0.0314],\n",
      "          [0.7255, 0.7333, 0.7373,  ..., 0.0471, 0.0392, 0.0314],\n",
      "          [0.8078, 0.8118, 0.8275,  ..., 0.0588, 0.0510, 0.0471],\n",
      "          ...,\n",
      "          [0.7529, 0.7804, 0.8235,  ..., 0.0196, 0.0196, 0.0314],\n",
      "          [0.6902, 0.7647, 0.8000,  ..., 0.0235, 0.0235, 0.0196],\n",
      "          [0.5804, 0.6863, 0.7529,  ..., 0.0353, 0.0275, 0.0157]]]]) tensor([2])\n",
      "torch.Size([1, 3, 64, 64]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(X, y)\n",
    "    print(X.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " [tensor([[[[0.0510, 0.0510, 0.0510,  ..., 0.0157, 0.0196, 0.0157],\n",
       "            [0.0863, 0.0941, 0.0980,  ..., 0.0196, 0.0196, 0.0157],\n",
       "            [0.0275, 0.0314, 0.0314,  ..., 0.0196, 0.0196, 0.0196],\n",
       "            ...,\n",
       "            [0.5176, 0.5333, 0.5490,  ..., 0.6118, 0.5961, 0.5843],\n",
       "            [0.5176, 0.5294, 0.5451,  ..., 0.5922, 0.5765, 0.5647],\n",
       "            [0.5020, 0.5176, 0.5333,  ..., 0.5647, 0.5490, 0.5373]],\n",
       "  \n",
       "           [[0.0431, 0.0431, 0.0431,  ..., 0.0157, 0.0196, 0.0157],\n",
       "            [0.0784, 0.0863, 0.0941,  ..., 0.0196, 0.0157, 0.0157],\n",
       "            [0.0196, 0.0275, 0.0314,  ..., 0.0157, 0.0118, 0.0157],\n",
       "            ...,\n",
       "            [0.5333, 0.5451, 0.5569,  ..., 0.5843, 0.5686, 0.5529],\n",
       "            [0.5216, 0.5333, 0.5451,  ..., 0.5608, 0.5451, 0.5255],\n",
       "            [0.5176, 0.5255, 0.5373,  ..., 0.5451, 0.5216, 0.5020]],\n",
       "  \n",
       "           [[0.0549, 0.0549, 0.0549,  ..., 0.0157, 0.0196, 0.0157],\n",
       "            [0.0902, 0.0980, 0.1020,  ..., 0.0196, 0.0196, 0.0157],\n",
       "            [0.0314, 0.0353, 0.0314,  ..., 0.0157, 0.0157, 0.0157],\n",
       "            ...,\n",
       "            [0.5765, 0.5725, 0.5725,  ..., 0.5686, 0.5412, 0.5216],\n",
       "            [0.5647, 0.5686, 0.5686,  ..., 0.5529, 0.5294, 0.5059],\n",
       "            [0.5490, 0.5529, 0.5569,  ..., 0.5373, 0.5098, 0.4902]]]]),\n",
       "  tensor([1])])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "next(enumerate(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02_pythorch-deep-learning-course-PstS3IoM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
