{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/07_pytorch_experiment_tracking_exercise_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"zNqPNlYylluR"},"source":["# 07. PyTorch Experiment Tracking Exercise Template\n","\n","Welcome to the 07. PyTorch Experiment Tracking exercise template notebook.\n","\n","> **Note:** There may be more than one solution to each of the exercises. This notebook only shows one possible example.\n","\n","## Resources\n","\n","1. These exercises/solutions are based on [section 07. PyTorch Transfer Learning](https://www.learnpytorch.io/07_pytorch_experiment_tracking/) of the Learn PyTorch for Deep Learning course by Zero to Mastery.\n","2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/cO_r2FYcAjU).\n","3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions).\n","\n","> **Note:** The first section of this notebook is dedicated to getting various helper functions and datasets used for the exercises. The exercises start at the heading \"Exercise 1: ...\"."]},{"cell_type":"markdown","source":["# Initialization\n","\n","Initially Colab has\n","\n","* torch version: 2.1.0+cu121\n","* torchvision version: 0.16.0+cu121\n","\n","Try the code bellow.\n","\n"],"metadata":{"id":"oszxALgP2ih_"}},{"cell_type":"code","source":["# # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n","\n","# import torch\n","# import torchvision\n","\n","# # Update code\n","# torch_version_str = '.'.join(torch.__version__.split('.')[0:2])\n","# torchvision_version_str = '.'.join(torchvision.__version__.split('.')[0:2])\n","# assert float(torch_version_str) >= 1.12, \"torch version should be 1.12+\"\n","# assert float(torchvision_version_str) >= 0.13, \"torchvision version should be 0.13+\"\n","\n","\n","# print(torch.__version__, torchvision.__version__)"],"metadata":{"id":"33fdZPUy0_Jp","executionInfo":{"status":"ok","timestamp":1704882421825,"user_tz":-180,"elapsed":287,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2138,"status":"ok","timestamp":1704882424169,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"UZh3ycy33Xiw","outputId":"d3cf0607-7368-4fac-f625-779462bde751"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":33}],"source":["# Check Colab Execution\n","\n","try:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","IN_COLAB"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6509,"status":"ok","timestamp":1704882430676,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"GRBmK8bDOrVL","outputId":"8c65a72a-a714-4389-b9ec-e2c2a6f3ad60"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.2)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.2)\n","Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.1)\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.16.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","CPU times: user 57 ms, sys: 2.1 ms, total: 59.1 ms\n","Wall time: 7.03 s\n"]}],"source":["%%time\n","\n","if IN_COLAB:\n","    # updgrade torch, there are some bugs with init weights in torch 2.1.0\n","    !pip install torch torchaudio torchdata torchtext torchvision -U\n","else:\n","    !pipenv install"]},{"cell_type":"markdown","metadata":{"id":"sf8ab9cyHTzU"},"source":["### Get various imports and helper functions\n","\n","We'll need to make sure we have `torch` v.1.12+ and `torchvision` v0.13+."]},{"cell_type":"code","source":["try:\n","    import torch\n","    import torchvision\n","\n","    # Update code\n","    torch_version_str = '.'.join(torch.__version__.split('.')[0:2])\n","    torchvision_version_str = '.'.join(torchvision.__version__.split('.')[0:2])\n","    assert float(torch_version_str) >= 1.12, \"torch version should be 1.12+\"\n","    assert float(torchvision_version_str) >= 0.13, \"torchvision version should be 0.13+\"\n","except:\n","  pass\n","\n","torch_version_str, torchvision_version_str"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3_M1W6a1qmx","executionInfo":{"status":"ok","timestamp":1704882431246,"user_tz":-180,"elapsed":13,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}},"outputId":"32cf4934-0981-4a31-8ce2-c56420f48a5e"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('2.1', '0.16')"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1704882431246,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"5MOv1De4mxeL","outputId":"707346d0-f8b8-4d76-e9b5-1e1cbac732b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch version: 2.1.2+cu121\n","torchvision version: 0.16.2+cu121\n","2.1.2+cu121 0.16.2+cu121\n"]}],"source":["# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n","try:\n","    import torch\n","    import torchvision\n","\n","    # Update code\n","    torch_version_str = '.'.join(torch.__version__.split('.')[0:2])\n","    torchvision_version_str = '.'.join(torchvision.__version__.split('.')[0:2])\n","    assert float(torch_version_str) >= 1.12, \"torch version should be 1.12+\"\n","    assert float(torchvision_version_str) >= 0.13, \"torchvision version should be 0.13+\"\n","\n","    # Previous code\n","    # assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n","    # assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n","    print(f\"torch version: {torch.__version__}\")\n","    print(f\"torchvision version: {torchvision.__version__}\")\n","except:\n","    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n","    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n","    import torch\n","    import torchvision\n","    print(f\"torch version: {torch.__version__}\")\n","    print(f\"torchvision version: {torchvision.__version__}\")\n","\n","print(torch.__version__, torchvision.__version__)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1704882431246,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"Nf-DsrZipCE9","outputId":"7ba10bc2-009a-4066-ac9a-8c1ae47114f4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}],"source":[" # Make sure we have a GPU\n"," device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"," device"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1704882431246,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"i_52puIeoab3"},"outputs":[],"source":["# Get regular imports\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","\n","from torch import nn\n","from torchvision import transforms\n","\n","# Try to get torchinfo, install it if it doesn't work\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","\n","# Try to import the going_modular directory, download it from GitHub if it doesn't work\n","try:\n","    from going_modular.going_modular import data_setup, engine\n","except:\n","    # Get the going_modular scripts\n","    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n","    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n","    !mv pytorch-deep-learning/going_modular .\n","    !rm -rf pytorch-deep-learning\n","    from going_modular.going_modular import data_setup, engine"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1704882431246,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"DBj8I3P9pNK2"},"outputs":[],"source":["# Set seeds\n","def set_seeds(seed: int=42):\n","    \"\"\"Sets random sets for torch operations.\n","\n","    Args:\n","        seed (int, optional): Random seed to set. Defaults to 42.\n","    \"\"\"\n","    # Set the seed for general torch operations\n","    torch.manual_seed(seed)\n","    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n","    torch.cuda.manual_seed(seed)"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1704882431246,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"B3hr9d5h3Xi3"},"outputs":[],"source":["# Get a summary of the model (uncomment for full output)\n","def get_summary(model, input_size=(32, 3, 224, 224)):\n","    print(summary(model,\n","            input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n","            verbose=0,\n","            col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","            col_width=20,\n","            row_settings=[\"var_names\"]\n","    ))"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1704882431246,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"m6R-CS53pTLS","outputId":"6eaab9a4-54c2-4c33-8862-865252186017"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] data/pizza_steak_sushi directory exists, skipping download.\n"]},{"output_type":"execute_result","data":{"text/plain":["PosixPath('data/pizza_steak_sushi')"]},"metadata":{},"execution_count":41}],"source":["# Download the data\n","import os\n","import zipfile\n","\n","from pathlib import Path\n","\n","import requests\n","\n","def download_data(source: str,\n","                  destination: str,\n","                  remove_source: bool = True) -> Path:\n","    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n","\n","    Args:\n","        source (str): A link to a zipped file containing data.\n","        destination (str): A target directory to unzip data to.\n","        remove_source (bool): Whether to remove the source after downloading and extracting.\n","\n","    Returns:\n","        pathlib.Path to downloaded data.\n","\n","    Example usage:\n","        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n","                      destination=\"pizza_steak_sushi\")\n","    \"\"\"\n","    # Setup path to data folder\n","    data_path = Path(\"data/\")\n","    image_path = data_path / destination\n","\n","    # If the image folder doesn't exist, download it and prepare it...\n","    if image_path.is_dir():\n","        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n","    else:\n","        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n","        image_path.mkdir(parents=True, exist_ok=True)\n","\n","        # Download pizza, steak, sushi data\n","        target_file = Path(source).name\n","        with open(data_path / target_file, \"wb\") as f:\n","            request = requests.get(source)\n","            print(f\"[INFO] Downloading {target_file} from {source}...\")\n","            f.write(request.content)\n","\n","        # Unzip pizza, steak, sushi data\n","        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n","            print(f\"[INFO] Unzipping {target_file} data...\")\n","            zip_ref.extractall(image_path)\n","\n","        # Remove .zip file\n","        if remove_source:\n","            os.remove(data_path / target_file)\n","\n","    return image_path\n","\n","image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n","                           destination=\"pizza_steak_sushi\")\n","image_path"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1704882431246,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"kKW0vtdYDdVL"},"outputs":[],"source":["if IN_COLAB:\n","  RUNS_DIR = Path('/content/drive/MyDrive/dev/data/01_python_data_packages/07_pythorch/02_pythorch-deep-learning-course/runs')\n","  MODELS_DIR = Path('/content/drive/MyDrive/dev/data/01_python_data_packages/07_pythorch/02_pythorch-deep-learning-course/models')\n","else:\n","  RUNS_DIR = Path('runs')\n","  MODELS_DIR = Path('models')\n"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1704882431246,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"BE60IEEkr89l"},"outputs":[],"source":["# Writer creation\n","from torch.utils.tensorboard import SummaryWriter\n","\n","\n","def create_writer(experiment_name: str,\n","                  model_name: str,\n","                  tag: str = None,\n","                  extra: str = None):\n","    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n","\n","    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n","\n","    Where timestamp is the current date in YYYY-MM-DD format.\n","\n","    Args:\n","        experiment_name (str): Name of experiment.\n","        model_name (str): Name of model.\n","        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n","\n","    Returns:\n","        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n","\n","    Example usage:\n","        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n","        writer = create_writer(experiment_name=\"data_10_percent\",\n","                               model_name=\"effnetb2\",\n","                               extra=\"5_epochs\")\n","        # The above is the same as:\n","        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n","    \"\"\"\n","    from datetime import datetime\n","    import os\n","\n","    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n","    # Get timestamp of current date (all experiments on certain day live in same folder)\n","    if tag:\n","        # returns current date in YYYY-MM-DD format\n","        timestamp = timestamp + '_' + tag\n","\n","    if extra:\n","        # Create log directory path\n","        log_dir = os.path.join(RUNS_DIR, timestamp,\n","                               experiment_name, model_name, extra)\n","    else:\n","        log_dir = os.path.join(RUNS_DIR, timestamp,\n","                               experiment_name, model_name)\n","\n","    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n","    return SummaryWriter(log_dir=log_dir)"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1704882431247,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"S0BH4ONGsgNB"},"outputs":[],"source":["# Create a test writer\n","# writer = create_writer(experiment_name=\"test_experiment_name\",\n","#                        model_name=\"this_is_the_model_name\",\n","#                        extra=\"add_a_little_extra_if_you_want\")"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1704882431247,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"VwO0Q1eFsusV"},"outputs":[],"source":["# Update train function with writer\n","from typing import Dict, List\n","from tqdm.auto import tqdm\n","\n","from going_modular.going_modular.engine import train_step, test_step\n","\n","# Add writer parameter to train()\n","def train(model: torch.nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          epochs: int,\n","          device: torch.device,\n","          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n","          ) -> Dict[str, List]:\n","    \"\"\"Trains and tests a PyTorch model.\n","\n","    Passes a target PyTorch models through train_step() and test_step()\n","    functions for a number of epochs, training and testing the model\n","    in the same epoch loop.\n","\n","    Calculates, prints and stores evaluation metrics throughout.\n","\n","    Stores metrics to specified writer log_dir if present.\n","\n","    Args:\n","      model: A PyTorch model to be trained and tested.\n","      train_dataloader: A DataLoader instance for the model to be trained on.\n","      test_dataloader: A DataLoader instance for the model to be tested on.\n","      optimizer: A PyTorch optimizer to help minimize the loss function.\n","      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n","      epochs: An integer indicating how many epochs to train for.\n","      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","      writer: A SummaryWriter() instance to log model results to.\n","\n","    Returns:\n","      A dictionary of training and testing loss as well as training and\n","      testing accuracy metrics. Each metric has a value in a list for\n","      each epoch.\n","      In the form: {train_loss: [...],\n","                train_acc: [...],\n","                test_loss: [...],\n","                test_acc: [...]}\n","      For example if training for epochs=2:\n","              {train_loss: [2.0616, 1.0537],\n","                train_acc: [0.3945, 0.3945],\n","                test_loss: [1.2641, 1.5706],\n","                test_acc: [0.3400, 0.2973]}\n","    \"\"\"\n","    # Create empty results dictionary\n","    results = {\"train_loss\": [],\n","               \"train_acc\": [],\n","               \"test_loss\": [],\n","               \"test_acc\": []\n","    }\n","\n","    # Loop through training and testing steps for a number of epochs\n","    for epoch in tqdm(range(epochs)):\n","        train_loss, train_acc = train_step(model=model,\n","                                          dataloader=train_dataloader,\n","                                          loss_fn=loss_fn,\n","                                          optimizer=optimizer,\n","                                          device=device)\n","        test_loss, test_acc = test_step(model=model,\n","          dataloader=test_dataloader,\n","          loss_fn=loss_fn,\n","          device=device)\n","\n","        # Print out what's happening\n","        print(\n","          f\"Epoch: {epoch+1} | \"\n","          f\"train_loss: {train_loss:.4f} | \"\n","          f\"train_acc: {train_acc:.4f} | \"\n","          f\"test_loss: {test_loss:.4f} | \"\n","          f\"test_acc: {test_acc:.4f}\"\n","        )\n","\n","        # Update results dictionary\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        results[\"test_loss\"].append(test_loss)\n","        results[\"test_acc\"].append(test_acc)\n","\n","\n","        ### New: Use the writer parameter to track experiments ###\n","        # See if there's a writer, if so, log to it\n","        if writer:\n","            # Add results to SummaryWriter\n","            writer.add_scalars(main_tag=\"Loss\",\n","                               tag_scalar_dict={\"train_loss\": train_loss,\n","                                                \"test_loss\": test_loss},\n","                               global_step=epoch)\n","            writer.add_scalars(main_tag=\"Accuracy\",\n","                               tag_scalar_dict={\"train_acc\": train_acc,\n","                                                \"test_acc\": test_acc},\n","                               global_step=epoch)\n","\n","            # Close the writer\n","            writer.close()\n","        else:\n","            pass\n","    ### End new ###\n","\n","    # Return the filled results at the end of the epochs\n","    return results"]},{"cell_type":"markdown","source":["### Start init here ctrl + f8"],"metadata":{"id":"kJU5gnOY6Kum"}},{"cell_type":"markdown","metadata":{"id":"nh8jKzHYHYL3"},"source":["### Download data\n","\n","Using the same data from https://www.learnpytorch.io/07_pytorch_experiment_tracking/"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1704880856620,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"68QGCR_1tzif"},"outputs":[],"source":["# Download 10 percent and 20 percent training data (if necessary)\n","data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n","                                     destination=\"pizza_steak_sushi\")\n","\n","data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n","                                     destination=\"pizza_steak_sushi_20_percent\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1704880856621,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"9L2rCRxvt1ED"},"outputs":[],"source":["# Setup training directory paths\n","train_dir_10_percent = data_10_percent_path / \"train\"\n","train_dir_20_percent = data_20_percent_path / \"train\"\n","\n","# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n","test_dir = data_10_percent_path / \"test\"\n","\n","# Check the directories\n","print(f\"Training directory 10%: {train_dir_10_percent}\")\n","print(f\"Training directory 20%: {train_dir_20_percent}\")\n","print(f\"Testing directory: {test_dir}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1704880856621,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"K35q9wswt6NH"},"outputs":[],"source":["# Creation transforms\n","from torchvision import transforms\n","\n","# Create a transform to normalize data distribution to be inline with ImageNet\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n","                                 std=[0.229, 0.224, 0.225])\n","\n","# Create a transform pipeline\n","simple_transform = transforms.Compose([\n","                                       transforms.Resize((224, 224)),\n","                                       transforms.ToTensor(), # get image values between 0 & 1\n","                                       normalize\n","])"]},{"cell_type":"markdown","metadata":{"id":"SBuEla8pHea9"},"source":["### Turn data into DataLoaders"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1704880856621,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"xlQU94HBuqOq"},"outputs":[],"source":["BATCH_SIZE = 32\n","\n","# Create 10% training and test DataLoaders\n","train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n","                                                                                          test_dir=test_dir,\n","                                                                                          transform=simple_transform,\n","                                                                                          batch_size=BATCH_SIZE)\n","\n","# Create 20% training and test DataLoaders\n","train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n","                                                                                          test_dir=test_dir,\n","                                                                                          transform=simple_transform,\n","                                                                                          batch_size=BATCH_SIZE)\n","\n","# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n","print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n","print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n","print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\n","print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"]},{"cell_type":"markdown","metadata":{"id":"ixiJc6lZ3Xi7"},"source":["### Create models"]},{"cell_type":"code","source":["import torchvision\n","from torch import nn\n","\n","# Get num out features (one for each class pizza, steak, sushi)\n","OUT_FEATURES = len(class_names)\n","\n","# Create an EffNetB0 feature extractor\n","def create_effnetb0():\n","    # 1. Get the base mdoel with pretrained weights and send to target device\n","    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n","    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n","\n","    # 2. Freeze the base model layers\n","    for param in model.features.parameters():\n","        param.requires_grad = False\n","\n","    # 3. Set the seeds\n","    set_seeds()\n","\n","    # 4. Change the classifier head\n","    model.classifier = nn.Sequential(\n","        nn.Dropout(p=0.2),\n","        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n","    ).to(device)\n","\n","    # 5. Give the model a name\n","    model.name = \"effnetb0\"\n","    print(f\"[INFO] Created new {model.name} model.\")\n","    return model\n","\n","# Create an EffNetB2 feature extractor\n","def create_effnetb2():\n","    # 1. Get the base model with pretrained weights and send to target device\n","    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n","    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n","\n","    # 2. Freeze the base model layers\n","    for param in model.features.parameters():\n","        param.requires_grad = False\n","\n","    # 3. Set the seeds\n","    set_seeds()\n","\n","    # 4. Change the classifier head\n","    model.classifier = nn.Sequential(\n","        nn.Dropout(p=0.3),\n","        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n","    ).to(device)\n","\n","    # 5. Give the model a name\n","    model.name = \"effnetb2\"\n","    print(f\"[INFO] Created new {model.name} model.\")\n","    return model\n","\n","# Create an EffNetB3 feature extractor\n","def create_effnetb3():\n","    # 1. Get the base mdoel with pretrained weights and send to target device\n","    weights = torchvision.models.EfficientNet_B3_Weights.DEFAULT\n","    print(device)\n","    model = torchvision.models.efficientnet_b3(weights=weights).to(device)\n","\n","    # 2. Freeze the base model layers\n","    for param in model.features.parameters():\n","        param.requires_grad = False\n","\n","    # 3. Set the seeds\n","    set_seeds()\n","\n","    # 4. Change the classifier head\n","    model.classifier = nn.Sequential(\n","        nn.Dropout(p=0.2),\n","        nn.Linear(in_features=1536, out_features=OUT_FEATURES)\n","    ).to(device)\n","\n","    # 5. Give the model a name\n","    model.name = \"effnetb3\"\n","    print(f\"[INFO] Created new {model.name} model.\")\n","    return model\n","\n","# Create an EffNetB5 feature extractor\n","def create_effnetb5():\n","    # 1. Get the base model with pretrained weights and send to target device\n","    weights = torchvision.models.EfficientNet_B5_Weights.DEFAULT\n","    model = torchvision.models.efficientnet_b5(weights=weights).to(device)\n","\n","    # 2. Freeze the base model layers\n","    for param in model.features.parameters():\n","        param.requires_grad = False\n","\n","    # 3. Set the seeds\n","    set_seeds()\n","\n","    # 4. Change the classifier head\n","    model.classifier = nn.Sequential(\n","        nn.Dropout(p=0.2),\n","        nn.Linear(in_features=2048, out_features=OUT_FEATURES)\n","    ).to(device)\n","\n","    # 5. Give the model a name\n","    model.name = \"effnetb5\"\n","    print(f\"[INFO] Created new {model.name} model.\")\n","    return model"],"metadata":{"id":"njxsRNAX3Xi7","executionInfo":{"status":"error","timestamp":1704880657695,"user_tz":-180,"elapsed":281,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}},"colab":{"base_uri":"https://localhost:8080/","height":390},"outputId":"57ffe4cb-a78b-4b2e-a8d1-343e1299c8a0"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name '_C' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-c75b686b2958>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get num out features (one for each class pizza, steak, sushi)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mOUT_FEATURES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodulefinder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32mraise\u001b[0m  \u001b[0;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"nwmoMhW8IqSu"},"source":["## Exercise 1: Pick a larger model from [`torchvision.models`](https://pytorch.org/vision/main/models.html) to add to the list of experiments (for example, EffNetB3 or higher)\n","\n","* How does it perform compared to our existing models?\n","* **Hint:** You'll need to set up an exerpiment similar to [07. PyTorch Experiment Tracking section 7.6](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code)."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"cs7YNRr3USnS","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1704880658209,"user_tz":-180,"elapsed":11,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}},"outputId":"f5ebe567-0452-41ab-acc4-395877e4aa16"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'create_effnetb0' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e72dd9b725d6>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 2. Create models list (need to create a new model for each experiment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# [\"effnetb3\", \"effnetb5\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"effnetb0\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_effnetb0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"effnetb2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_effnetb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"effnetb3\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_effnetb3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'create_effnetb0' is not defined"]}],"source":["# 1. Create epochs list\n","num_epochs = [5, 10]\n","\n","# 2. Create models list (need to create a new model for each experiment)\n","models = {}  # [\"effnetb3\", \"effnetb5\"]\n","models[\"effnetb0\"] = create_effnetb0\n","models[\"effnetb2\"] = create_effnetb2\n","models[\"effnetb3\"] = create_effnetb3\n","models[\"effnetb5\"] = create_effnetb5\n","\n","# 3. Create dataloaders dictionary for various dataloaders\n","train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n","                     \"data_20_percent\": train_dataloader_20_percent}"]},{"cell_type":"markdown","source":["%%time\n","from going_modular.going_modular.utils import save_model\n","\n","# 1. Set the random seeds\n","set_seeds(seed=42)\n","\n","# 2. Keep track of experiment numbers\n","experiment_number = 0\n","\n","# 3. Loop through each DataLoader\n","for dataloader_name, train_dataloader in train_dataloaders.items():\n","\n","    # 4. Loop through each number of epochs\n","    for epochs in num_epochs:\n","\n","        # 5. Loop through each model name and create a new model based on the name\n","        for model_name, model_init in models.items():\n","\n","            # 6. Create information print outs\n","            experiment_number += 1\n","            print(f\"[INFO] Experiment number: {experiment_number}\")\n","            print(f\"[INFO] Model: {model_name}\")\n","            print(f\"[INFO] DataLoader: {dataloader_name}\")\n","            print(f\"[INFO] Number of epochs: {epochs}\")\n","\n","            # 7. Select the model\n","            # if model_name == \"effnetb0\":\n","            #     model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n","            # else:\n","            #     model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n","\n","            # break\n","            model = model_init()\n","\n","            # 8. Create a new loss and optimizer for every model\n","            loss_fn = nn.CrossEntropyLoss()\n","            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n","\n","            # 9. Train target model with target dataloaders and track experiments\n","            train(model=model,\n","                  train_dataloader=train_dataloader,\n","                  test_dataloader=test_dataloader,\n","                  optimizer=optimizer,\n","                  loss_fn=loss_fn,\n","                  epochs=epochs,\n","                  device=device,\n","                  writer=create_writer(experiment_name=dataloader_name,\n","                                       model_name=model_name,\n","                                       extra=f\"{epochs}_epochs\"))\n","\n","            # 10. Save the model to file so we can get back the best model\n","            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n","            save_model(model=model,\n","                       target_dir=MODELS_DIR,\n","                       model_name=save_filepath)\n","            print(\"-\"*50 + \"\\n\")"],"metadata":{"id":"ttgZdwBUov9C"}},{"cell_type":"code","source":["1 /0"],"metadata":{"id":"Hi34y69SmPh2","executionInfo":{"status":"aborted","timestamp":1704880658209,"user_tz":-180,"elapsed":10,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YqlStPo-gbrF"},"source":["## Exercise 2. Introduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?\n","    \n","* For example, you could have one training DataLoader that uses data augmentation (e.g. `train_dataloader_20_percent_aug` and `train_dataloader_20_percent_no_aug`) and then compare the results of two of the same model types training on these two DataLoaders.\n","* **Note:** You may need to alter the `create_dataloaders()` function to be able to take a transform for the training data and the testing data (because you don't need to perform data augmentation on the test data). See [04. PyTorch Custom Datasets section 6](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation) for examples of using data augmentation or the script below for an example:\n","\n","```python\n","# Note: Data augmentation transform like this should only be performed on training data\n","train_transform_data_aug = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.TrivialAugmentWide(),\n","    transforms.ToTensor(),\n","    normalize\n","])\n","\n","# Create a helper function to visualize different augmented (and not augmented) images\n","def view_dataloader_images(dataloader, n=10):\n","    if n > 10:\n","        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n","        n = 10\n","    imgs, labels = next(iter(dataloader))\n","    plt.figure(figsize=(16, 8))\n","    for i in range(n):\n","        # Min max scale the image for display purposes\n","        targ_image = imgs[i]\n","        sample_min, sample_max = targ_image.min(), targ_image.max()\n","        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n","\n","        # Plot images with appropriate axes information\n","        plt.subplot(1, 10, i+1)\n","        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n","        plt.title(class_names[labels[i]])\n","        plt.axis(False)\n","\n","# Have to update `create_dataloaders()` to handle different augmentations\n","import os\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","\n","NUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data\n","\n","# Note: this is an update version of data_setup.create_dataloaders to handle\n","# differnt train and test transforms.\n","def create_dataloaders(\n","    train_dir,\n","    test_dir,\n","    train_transform, # add parameter for train transform (transforms on train dataset)\n","    test_transform,  # add parameter for test transform (transforms on test dataset)\n","    batch_size=32, num_workers=NUM_WORKERS\n","):\n","    # Use ImageFolder to create dataset(s)\n","    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n","    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n","\n","    # Get class names\n","    class_names = train_data.classes\n","\n","    # Turn images into data loaders\n","    train_dataloader = DataLoader(\n","        train_data,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","    )\n","    test_dataloader = DataLoader(\n","        test_data,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","    )\n","\n","    return train_dataloader, test_dataloader, class_names\n","```"]},{"cell_type":"markdown","metadata":{"id":"2bBt14QM3Xi9"},"source":["### AUG transform\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-VV18hew3Xi9","executionInfo":{"status":"aborted","timestamp":1704880658210,"user_tz":-180,"elapsed":11,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}}},"outputs":[],"source":["train_transform_data_aug = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.TrivialAugmentWide(),\n","    transforms.ToTensor(),\n","    normalize\n","])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"y0oXIogU3Xi9","colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"status":"error","timestamp":1704880658210,"user_tz":-180,"elapsed":11,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}},"outputId":"1619db19-8a80-493b-bc16-a9f5fd31a08b"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name '_C' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d5242bacf999>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodulefinder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32mraise\u001b[0m  \u001b[0;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"]}],"source":["# view dataloader images func\n","# Create a helper function to visualize different augmented (and not augmented) images\n","def view_dataloader_images(dataloader, n=10):\n","    if n > 10:\n","        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n","        n = 10\n","    imgs, labels = next(iter(dataloader))\n","    plt.figure(figsize=(16, 8))\n","    for i in range(n):\n","        # Min max scale the image for display purposes\n","        targ_image = imgs[i]\n","        sample_min, sample_max = targ_image.min(), targ_image.max()\n","        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n","\n","        # Plot images with appropriate axes information\n","        plt.subplot(1, 10, i+1)\n","        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n","        plt.title(class_names[labels[i]])\n","        plt.axis(False)\n","\n","# Have to update `create_dataloaders()` to handle different augmentations\n","import os\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n"]},{"cell_type":"markdown","metadata":{"id":"6EI78x_83Xi9"},"source":["### New Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2GQwdeJ3Xi9","executionInfo":{"status":"aborted","timestamp":1704880658210,"user_tz":-180,"elapsed":10,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}}},"outputs":[],"source":["# New version of dataloaders.\n","# Note: this is an update version of data_setup.create_dataloaders to handle\n","# differnt train and test transforms.\n","\n","NUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data\n","\n","def create_dataloaders(\n","    train_dir,\n","    test_dir,\n","    train_transform, # add parameter for train transform (transforms on train dataset)\n","    test_transform,  # add parameter for test transform (transforms on test dataset)\n","    batch_size=32, num_workers=NUM_WORKERS\n","):\n","    # Use ImageFolder to create dataset(s)\n","    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n","    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n","\n","    # Get class names\n","    class_names = train_data.classes\n","\n","    # Turn images into data loaders\n","    train_dataloader = DataLoader(\n","        train_data,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","    )\n","    test_dataloader = DataLoader(\n","        test_data,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","    )\n","\n","    return train_dataloader, test_dataloader, class_names"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704880658210,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"pB4iGbGj3Xi-"},"outputs":[],"source":["BATCH_SIZE = 32\n","\n","# Create 20% training and test DataLoaders\n","train_dataloader_20_percent_aug, test_dataloader, class_names = create_dataloaders(train_dir=train_dir_20_percent,\n","                                                                                   test_dir=test_dir,\n","                                                                                   train_transform=train_transform_data_aug,\n","                                                                                   test_transform=simple_transform,\n","                                                                                   batch_size=BATCH_SIZE)\n","\n","# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n","print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data aug: {len(train_dataloader_20_percent_aug)}\")\n","print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"]},{"cell_type":"markdown","metadata":{"id":"TWXKM6aSSNxs"},"source":["### Setup experiment. AUG dataloader Effnetb3, effnetb5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1N3yyDOoH2t","executionInfo":{"status":"aborted","timestamp":1704880658210,"user_tz":-180,"elapsed":10,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}}},"outputs":[],"source":["# TODO: your code\n","\n","# 1. Create epochs list\n","num_epochs = [5, 10]\n","\n","# 2. Create models list (need to create a new model for each experiment)\n","models = {} #[\"effnetb3\", \"effnetb5\"]\n","models[\"effnetb0\"] = create_effnetb0\n","models[\"effnetb2\"] = create_effnetb2\n","models[\"effnetb3\"] = create_effnetb3\n","models[\"effnetb5\"] = create_effnetb5\n","\n","train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n","                     \"data_20_percent\": train_dataloader_20_percent,\n","                     \"data_20_percent_aug\": train_dataloader_20_percent_aug}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704880658210,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"OsZiH75o3Xi-"},"outputs":[],"source":["view_dataloader_images(train_dataloader_20_percent_aug)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704880658210,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"Zvb1dyZP3Xi-"},"outputs":[],"source":["view_dataloader_images(train_dataloader_20_percent)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704880658210,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"J9w6cr3b3Xi-"},"outputs":[],"source":["%%time\n","from going_modular.going_modular.utils import save_model\n","\n","# 1. Set the random seeds\n","set_seeds(seed=42)\n","\n","# 2. Keep track of experiment numbers\n","experiment_number = 0\n","\n","# 3. Loop through each DataLoader\n","for dataloader_name, train_dataloader in train_dataloaders.items():\n","\n","    # 4. Loop through each number of epochs\n","    for epochs in num_epochs:\n","\n","        # 5. Loop through each model name and create a new model based on the name\n","        for model_name, model_init in models.items():\n","\n","            # 6. Create information print outs\n","            experiment_number += 1\n","            print(f\"[INFO] Experiment number: {experiment_number}\")\n","            print(f\"[INFO] Model: {model_name}\")\n","            print(f\"[INFO] DataLoader: {dataloader_name}\")\n","            print(f\"[INFO] Number of epochs: {epochs}\")\n","            print(f\"[INFO] Number of epochs: {epochs}\")\n","\n","            # 7. Select the model\n","            # if model_name == \"effnetb0\":\n","            #     model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n","            # else:\n","            #     model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n","\n","            # break\n","            model = model_init()\n","            # 8. Create a new loss and optimizer for every model\n","            loss_fn = nn.CrossEntropyLoss()\n","            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n","\n","            # 9. Train target model with target dataloaders and track experiments\n","            train(model=model,\n","                  train_dataloader=train_dataloader,\n","                  test_dataloader=test_dataloader,\n","                  optimizer=optimizer,\n","                  loss_fn=loss_fn,\n","                  epochs=epochs,\n","                  device=device,\n","                  writer=create_writer(experiment_name=dataloader_name,\n","                                       model_name=model_name,\n","                                       extra=f\"{epochs}_epochs\"))\n","\n","            # 10. Save the model to file so we can get back the best model\n","            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n","            save_model(model=model,\n","                       target_dir=MODELS_DIR,\n","                       model_name=save_filepath)\n","            print(\"-\"*50 + \"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkhdICxGL-cV","executionInfo":{"status":"aborted","timestamp":1704880658210,"user_tz":-180,"elapsed":9,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}}},"outputs":[],"source":["# TODO: your code\n","\n","# 1. Create epochs list\n","num_epochs = [5, 10]\n","\n","# 2. Create models list (need to create a new model for each experiment)\n","models = {} #[\"effnetb3\", \"effnetb5\"]\n","models[\"effnetb0\"] = create_effnetb0\n","models[\"effnetb2\"] = create_effnetb2\n","models[\"effnetb3\"] = create_effnetb3\n","models[\"effnetb5\"] = create_effnetb5\n","\n","# 3. Create dataloaders dictionary for various dataloaders\n","# train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n","#                      \"data_20_percent\": train_dataloader_20_percent}\n","\n","num_lr = [0.001, 0.0001]\n","\n","train_dataloaders = {\"data_20_percent_aug\": train_dataloader_20_percent_aug}\n"]},{"cell_type":"markdown","metadata":{"id":"ernW6I3tSNxt"},"source":["### Exp 2.1 Changing learning rate"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1704880658211,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"Nlzkbi2YL-cV"},"outputs":[],"source":["%%time\n","from going_modular.going_modular.utils import save_model\n","\n","# 1. Set the random seeds\n","set_seeds(seed=42)\n","\n","# 2. Keep track of experiment numbers\n","experiment_number = 0\n","\n","# 3. Loop through each DataLoader\n","for dataloader_name, train_dataloader in train_dataloaders.items():\n","\n","    # 4. Loop through each number of epochs\n","    for epochs in num_epochs:\n","\n","        # 5. Loop through each model name and create a new model based on the name\n","        for model_name, model_init in models.items():\n","\n","            for lr in num_lr:\n","\n","                # 6. Create information print outs\n","                experiment_number += 1\n","                print(f\"[INFO] Experiment number: {experiment_number}\")\n","                print(f\"[INFO] Model: {model_name}\")\n","                print(f\"[INFO] DataLoader: {dataloader_name}\")\n","                print(f\"[INFO] Number of epochs: {epochs}\")\n","                print(f\"[INFO] Number of epochs: {lr}\")\n","\n","\n","                # 7. Select the model\n","                # if model_name == \"effnetb0\":\n","                #     model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n","                # else:\n","                #     model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n","\n","                # break\n","                model = model_init()\n","                # 8. Create a new loss and optimizer for every model\n","                loss_fn = nn.CrossEntropyLoss()\n","                optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n","\n","                # 9. Train target model with target dataloaders and track experiments\n","                train(model=model,\n","                    train_dataloader=train_dataloader,\n","                    test_dataloader=test_dataloader,\n","                    optimizer=optimizer,\n","                    loss_fn=loss_fn,\n","                    epochs=epochs,\n","                    device=device,\n","                    writer=create_writer(experiment_name=dataloader_name,\n","                                        model_name=model_name,\n","                                        tag='lr',\n","                                        extra=f\"{epochs}_epochs_{lr}_learning_rate\"))\n","\n","                # 10. Save the model to file so we can get back the best model\n","                save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs_{lr}_learning_rate.pth\"\n","                save_model(model=model,\n","                        target_dir=MODELS_DIR,\n","                        model_name=save_filepath)\n","                print(\"-\"*50 + \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"1IvuTskxgjaw"},"source":["## Exercise 3. Scale up the dataset to turn FoodVision Mini into FoodVision Big using the entire [Food101 dataset from `torchvision.models`](https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html#torchvision.datasets.Food101)\n","    \n","* You could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.\n","* If you try more than one model, it would be good to have the model's results tracked.\n","* If you load the Food101 dataset from `torchvision.models`, you'll have to create PyTorch DataLoaders to use it in training.\n","* **Note:** Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train."]},{"cell_type":"markdown","source":["### Preapare data"],"metadata":{"id":"KlrvYDDe5KaY"}},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":310525,"status":"ok","timestamp":1704882773654,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"},"user_tz":-180},"id":"YehliYnYoP1x","colab":{"base_uri":"https://localhost:8080/"},"outputId":"df1d0a90-39ca-4d46-990c-c03585000235"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz to data_food/food-101.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 4996278331/4996278331 [03:37<00:00, 22964495.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data_food/food-101.tar.gz to data_food\n"]}],"source":["# TODO: your code\n","\n","import torchvision\n","\n","# Create a transform to normalize data distribution to be inline with ImageNet\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n","                                 std=[0.229, 0.224, 0.225])\n","\n","# Create a transform pipeline\n","simple_transform = transforms.Compose([\n","                                       transforms.Resize((224, 224)),\n","                                       transforms.ToTensor(), # get image values between 0 & 1\n","                                       normalize\n","])\n","\n","train_data = torchvision.datasets.Food101(root='data_food', split='train', transform=simple_transform, download=True)\n","test_data = torchvision.datasets.Food101(root='data_food', split='test', transform=simple_transform, download=True)\n"]},{"cell_type":"code","source":["len(train_data), len(test_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dpv-cWYbuTa3","executionInfo":{"status":"ok","timestamp":1704882773654,"user_tz":-180,"elapsed":3,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}},"outputId":"76a11734-fd38-4730-a139-9db83559f56d"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(75750, 25250)"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["import os\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","BATCH_SIZE = 32\n","NUM_WORKERS = os.cpu_count()\n","\n","train_dataloader_food = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n","test_dataloader_food = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n","NUM_WORKERS"],"metadata":{"id":"GgrN_HYfA1sa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704883005526,"user_tz":-180,"elapsed":205,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}},"outputId":"4f484377-a405-4c2c-aec5-d4ccf03dd08e"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"qMHCtqUTyXHt","executionInfo":{"status":"ok","timestamp":1704883008791,"user_tz":-180,"elapsed":219,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}},"outputId":"28c4caf4-d74f-4ca6-a91d-52e27d72c396"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["### Prepare model\n","The most efficient NN was EffNetb2."],"metadata":{"id":"0X0NpWRypRlp"}},{"cell_type":"code","source":["model_weight = torchvision.models.EfficientNet_B2_Weights.IMAGENET1K_V1\n","big_model = torchvision.models.efficientnet_b2(weights=model_weight).to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZh5SvZ6sJgE","executionInfo":{"status":"ok","timestamp":1704883014386,"user_tz":-180,"elapsed":1642,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}},"outputId":"70358ca4-8a91-48b3-c73f-76b21b8212f1"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n","100%|| 35.2M/35.2M [00:00<00:00, 132MB/s]\n"]}]},{"cell_type":"code","source":["for param in big_model.features.parameters():\n","  param.requires_grad = False\n","\n","\n","big_model.classifier = nn.Sequential(\n","    nn.Dropout(p=0.2),\n","    nn.Linear(in_features=1408, out_features=101)\n",").to(device)"],"metadata":{"id":"Jrd9NyQ4vi5T","executionInfo":{"status":"ok","timestamp":1704883035068,"user_tz":-180,"elapsed":226,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}}},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["### Train big model\n"],"metadata":{"id":"cO0s2vwzuc-e"}},{"cell_type":"code","source":["%%time\n","epochs = 5\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params=big_model.parameters(), lr=0.001)\n","\n","train(model=big_model,\n","      train_dataloader=train_dataloader_food,\n","      test_dataloader=test_dataloader_food,\n","      optimizer=optimizer,\n","      loss_fn=loss_fn,\n","      epochs=epochs,\n","      device=device,\n","      writer=create_writer(experiment_name='food_data',\n","                            model_name='big_model',\n","                            extra=f\"{epochs}_epochs\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535,"referenced_widgets":["3f1f76498b204a5cab9ecee44fccdd9b","a8a83e16aa134edaaa5698a43ebc70e1","94be1fb70f5046e08ea48b67674be96a","fbed1c435ec5440d8099795d0178459b","fa048756647b41d9b6e87bf5737fa10c","8226e399569048a29020124529616d66","6ec2f22059194e50bfd005f4f7eee1ae","5797ff0646834986b3e56b96b045d85b","f3ddd30f760c469ab28829a84bb37167","0f49afa7f827495bab18a2a4c52b7e0f","c3d276f111fd4facba77ad55841f60c5"]},"id":"BL6h_87Kti0o","executionInfo":{"status":"ok","timestamp":1704886111716,"user_tz":-180,"elapsed":2752797,"user":{"displayName":"Anton Troitsky","userId":"10719431678441686853"}},"outputId":"64170e80-4fb1-4729-84e3-39976288366b"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Created SummaryWriter, saving to: /content/drive/MyDrive/dev/data/01_python_data_packages/07_pythorch/02_pythorch-deep-learning-course/runs/2024-01-10/food_data/big_model/5_epochs...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1f76498b204a5cab9ecee44fccdd9b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | train_loss: 2.2210 | train_acc: 0.4613 | test_loss: 1.8343 | test_acc: 0.5454\n","Epoch: 2 | train_loss: 2.0840 | train_acc: 0.4908 | test_loss: 1.7907 | test_acc: 0.5485\n","Epoch: 3 | train_loss: 2.0251 | train_acc: 0.4989 | test_loss: 1.7418 | test_acc: 0.5596\n","Epoch: 4 | train_loss: 1.9953 | train_acc: 0.5063 | test_loss: 1.7420 | test_acc: 0.5592\n","Epoch: 5 | train_loss: 1.9751 | train_acc: 0.5091 | test_loss: 1.7387 | test_acc: 0.5612\n","CPU times: user 12min 30s, sys: 2min 10s, total: 14min 40s\n","Wall time: 45min 52s\n"]},{"output_type":"execute_result","data":{"text/plain":["{'train_loss': [2.2209941292231954,\n","  2.0840277220751786,\n","  2.0251466219086907,\n","  1.9953476098693304,\n","  1.9751084891173083],\n"," 'train_acc': [0.4612718186936937,\n","  0.4907930391328829,\n","  0.4988650760135135,\n","  0.5062596776463965,\n","  0.509057397240991],\n"," 'test_loss': [1.8342860638246505,\n","  1.7906586584976958,\n","  1.7418294933211955,\n","  1.742007391239646,\n","  1.7387448701111576],\n"," 'test_acc': [0.5454113924050633,\n","  0.5485363924050632,\n","  0.5596123417721519,\n","  0.5591772151898734,\n","  0.5611946202531646]}"]},"metadata":{},"execution_count":57}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3f1f76498b204a5cab9ecee44fccdd9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8a83e16aa134edaaa5698a43ebc70e1","IPY_MODEL_94be1fb70f5046e08ea48b67674be96a","IPY_MODEL_fbed1c435ec5440d8099795d0178459b"],"layout":"IPY_MODEL_fa048756647b41d9b6e87bf5737fa10c"}},"a8a83e16aa134edaaa5698a43ebc70e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8226e399569048a29020124529616d66","placeholder":"","style":"IPY_MODEL_6ec2f22059194e50bfd005f4f7eee1ae","value":"100%"}},"94be1fb70f5046e08ea48b67674be96a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5797ff0646834986b3e56b96b045d85b","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f3ddd30f760c469ab28829a84bb37167","value":5}},"fbed1c435ec5440d8099795d0178459b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f49afa7f827495bab18a2a4c52b7e0f","placeholder":"","style":"IPY_MODEL_c3d276f111fd4facba77ad55841f60c5","value":" 5/5 [45:52&lt;00:00, 546.73s/it]"}},"fa048756647b41d9b6e87bf5737fa10c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8226e399569048a29020124529616d66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ec2f22059194e50bfd005f4f7eee1ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5797ff0646834986b3e56b96b045d85b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3ddd30f760c469ab28829a84bb37167":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f49afa7f827495bab18a2a4c52b7e0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3d276f111fd4facba77ad55841f60c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}